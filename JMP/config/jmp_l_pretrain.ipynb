{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "id='sqjvgggl' trainer=TrainerConfig(optimizer=OptimizerConfig(log_grad_norm=True, gradient_clipping=GradientClippingConfig(value=1.0)), supports_skip_batch_exception=False, supports_parameter_hooks=False, set_float32_matmul_precision='medium', precision='16-mixed', use_distributed_sampler=False) optimizer=AdamWConfig(lr=0.0003, weight_decay=0.1, betas=(0.9, 0.95)) lr_scheduler=LinearWarmupCosineAnnealingSchedulerConfig(warmup_steps=2000, max_epochs=2, warmup_start_lr_factor=0.2, min_lr_factor=0.1) edge_dropout=0.1 backbone=BackboneConfig(num_spherical=7, num_radial=128, num_blocks=6, emb_size_atom=256, emb_size_edge=1024, emb_size_trip_in=64, emb_size_trip_out=128, emb_size_quad_in=64, emb_size_quad_out=32, emb_size_aint_in=64, emb_size_aint_out=64, emb_size_rbf=32, emb_size_cbf=16, emb_size_sbf=64, num_before_skip=2, num_after_skip=2, num_concat=4, num_atom=3, num_output_afteratom=3, num_atom_emb_layers=2, direct_forces=True, sbf={'name': 'legendre_outer'}, quad_interaction=True, atom_edge_interaction=True, edge_atom_interaction=True, atom_interaction=True, qint_tags=[1, 2], absolute_rbf_cutoff=12.0, dropout=None, edge_dropout=0.1) batch_size=4 num_workers=8 tasks=[TaskConfig(name='transition1x', train_dataset=PretrainDatasetConfig(src=PosixPath('/shared/pre-training-datasets/trans1x/train'), metadata_path=PosixPath('/shared/pre-training-datasets/trans1x/train/metadata.npz')), val_dataset=PretrainDatasetConfig(src=PosixPath('/shared/pre-training-datasets/trans1x/val'), metadata_path=PosixPath('/shared/pre-training-datasets/trans1x/val/metadata.npz')), force_loss_scale=14.0, normalization={'y': NormalizationConfig(mean=0.0, std=1.787466168382901), 'force': NormalizationConfig(mean=0.0, std=0.3591422140598297)})] mt_dataset=MTDatasetConfig(sample_type='temperature', sample_temperature=2.0) ema=EMAConfig(decay=0.99)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/sanjeevr/MLFF-distill/JMP/src/jmp/lightning/model/config.py:718: BaseConfig._rng is None. The generated IDs will not be reproducible. To fix this, call BaseConfig.set_seed(...) before generating any IDs.\n"
                    ]
                }
            ],
            "source": [
                "\"\"\"\n",
                "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
                "All rights reserved.\n",
                "\n",
                "This source code is licensed under the license found in the\n",
                "LICENSE file in the root directory of this source tree.\n",
                "\"\"\"\n",
                "\n",
                "from pathlib import Path\n",
                "\n",
                "from jmp.configs.pretrain.jmp_l import jmp_l_pt_config_\n",
                "from jmp.tasks.pretrain import PretrainConfig, PretrainModel\n",
                "from jmp.tasks.pretrain.module import (\n",
                "    NormalizationConfig,\n",
                "    PretrainDatasetConfig,\n",
                "    TaskConfig,\n",
                ")\n",
                "\n",
                "\n",
                "# Let's make the config\n",
                "def jmp_l_config():\n",
                "    config = PretrainConfig.draft()\n",
                "\n",
                "    jmp_l_pt_config_(config)\n",
                "\n",
                "    # Set data config\n",
                "    config.batch_size = 4\n",
                "    config.num_workers = 8\n",
                "\n",
                "    # Set the tasks\n",
                "    config.tasks = [\n",
                "        # TaskConfig(\n",
                "        #     name=\"oc20\",\n",
                "        #     train_dataset=PretrainDatasetConfig(\n",
                "        #         src=Path(\"/datasets/s2ef/2M/train/\"),\n",
                "        #         metadata_path=Path(\"/datasets/s2ef/2M/train_metadata.npz\"),\n",
                "        #     ),\n",
                "        #     val_dataset=PretrainDatasetConfig(\n",
                "        #         src=Path(\"/datasets/s2ef/all/val_id/\"),\n",
                "        #         metadata_path=Path(\"/datasets/s2ef/all/val_id_metadata.npz\"),\n",
                "        #     ),\n",
                "        #     energy_loss_scale=1.0,\n",
                "        #     force_loss_scale=73.0,\n",
                "        #     normalization={\n",
                "        #         \"y\": NormalizationConfig(mean=0.0, std=24.901469505465872),\n",
                "        #         \"force\": NormalizationConfig(mean=0.0, std=0.5111534595489502),\n",
                "        #     },\n",
                "        # ),\n",
                "        # TaskConfig(\n",
                "        #     name=\"oc22\",\n",
                "        #     train_dataset=PretrainDatasetConfig(\n",
                "        #         src=Path(\"/shared/pre-training-datasets/oc22/s2ef-total/train/\"),\n",
                "        #     ),\n",
                "        #     val_dataset=PretrainDatasetConfig(\n",
                "        #         src=Path(\"/shared/pre-training-datasets/oc22/s2ef-total/val_id/\"),\n",
                "        #     ),\n",
                "        #     energy_loss_scale=1.0,\n",
                "        #     force_loss_scale=80.0,\n",
                "        #     normalization={\n",
                "        #         \"y\": NormalizationConfig(mean=0.0, std=25.229595396538468),\n",
                "        #         \"force\": NormalizationConfig(mean=0.0, std=0.25678861141204834),\n",
                "        #     },\n",
                "        # ),\n",
                "        # TaskConfig(\n",
                "        #     name=\"ani1x\",\n",
                "        #     train_dataset=PretrainDatasetConfig(\n",
                "        #         src=Path(\"/shared/pre-training-datasets/ani1x/train/\"),\n",
                "        #     ),\n",
                "        #     val_dataset=PretrainDatasetConfig(\n",
                "        #         src=Path(\"/shared/pre-training-datasets/ani1x/val/\"),\n",
                "        #     ),\n",
                "        #     energy_loss_scale=1.0,\n",
                "        #     force_loss_scale=15.0,\n",
                "        #     normalization={\n",
                "        #         \"y\": NormalizationConfig(mean=0.0, std=2.8700712783472118),\n",
                "        #         \"force\": NormalizationConfig(mean=0.0, std=2.131422996520996),\n",
                "        #     },\n",
                "        # ),\n",
                "        TaskConfig(\n",
                "            name=\"transition1x\",\n",
                "            train_dataset=PretrainDatasetConfig(\n",
                "                src=Path(\"/shared/pre-training-datasets/trans1x/train/\"),\n",
                "            ),\n",
                "            val_dataset=PretrainDatasetConfig(\n",
                "                src=Path(\"/shared/pre-training-datasets/trans1x/val/\"),\n",
                "            ),\n",
                "            energy_loss_scale=1.0,\n",
                "            force_loss_scale=14.0,\n",
                "            normalization={\n",
                "                \"y\": NormalizationConfig(mean=0.0, std=1.787466168382901),\n",
                "                \"force\": NormalizationConfig(mean=0.0, std=0.3591422140598297),\n",
                "            },\n",
                "        ),\n",
                "    ]\n",
                "\n",
                "    return config.finalize()\n",
                "\n",
                "\n",
                "config = jmp_l_config()\n",
                "print(config)\n",
                "\n",
                "configs: list[tuple[PretrainConfig, type[PretrainModel]]] = []\n",
                "configs.append((config, PretrainModel))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "63affeeb0caa4920977e93c9f9f5bf83",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Fast dev run:   0%|          | 0/1 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Failed to import lovely-tensors. Ignoring pretty PyTorch tensor formatting\n",
                        "Failed to import rich. Falling back to default Python logging.\n",
                        "CRITICAL:jmp.lightning.trainer.trainer:Setting config.trainer.default_root_dir='/home/sanjeevr/MLFF-distill/JMP/config/lightning_logs/89nm5lk0'.\n",
                        "Seed set to 0\n",
                        "CRITICAL:jmp.lightning.util.seed:Set global seed to 0.\n",
                        "CRITICAL:jmp.lightning.runner:Auto-wrapping run in Trainer context\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Unrecognized arguments:  dict_keys(['learnable_rbf', 'learnable_rbf_stds', 'unique_basis_per_layer', 'dropout', 'edge_dropout'])\n",
                        "> \u001b[0;32m/tmp/ipykernel_3045647/744631729.py\u001b[0m(9)\u001b[0;36mrun\u001b[0;34m()\u001b[0m\n",
                        "\u001b[0;32m      7 \u001b[0;31m    \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0m\u001b[0;32m      8 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0m\u001b[0;32m----> 9 \u001b[0;31m    \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0m\u001b[0;32m     10 \u001b[0;31m    \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0m\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0m\n",
                        "220879104\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "CRITICAL:jmp.lightning.trainer.trainer:Ran 0 finalizers for Trainer cleanup.\n",
                        "CRITICAL:jmp.lightning.util.seed:Reset global seed.\n",
                        "CRITICAL:jmp.lightning.runner:Error in run with run_id='sqjvgggl' (run_name=None): \n",
                        "Traceback (most recent call last):\n",
                        "  File \"/home/sanjeevr/MLFF-distill/JMP/src/jmp/lightning/runner.py\", line 456, in fast_dev_run\n",
                        "    return_value = self.local((config, *args), env=env, reset_id=True)\n",
                        "  File \"/home/sanjeevr/.local/lib/python3.10/site-packages/typing_extensions.py\", line 2853, in wrapper\n",
                        "    return arg(*args, **kwargs)\n",
                        "  File \"/home/sanjeevr/MLFF-distill/JMP/src/jmp/lightning/runner.py\", line 198, in local\n",
                        "    return_value = self._run_fn(config, *args)\n",
                        "  File \"/home/sanjeevr/MLFF-distill/JMP/src/jmp/lightning/runner.py\", line 120, in wrapped_run\n",
                        "    return run(config, *args)\n",
                        "  File \"/tmp/ipykernel_3045647/744631729.py\", line 9, in run\n",
                        "    trainer = Trainer(config)\n",
                        "  File \"/tmp/ipykernel_3045647/744631729.py\", line 9, in run\n",
                        "    trainer = Trainer(config)\n",
                        "  File \"/home/sanjeevr/miniforge3/envs/egap/lib/python3.10/bdb.py\", line 90, in trace_dispatch\n",
                        "    return self.dispatch_line(frame)\n",
                        "  File \"/home/sanjeevr/miniforge3/envs/egap/lib/python3.10/bdb.py\", line 115, in dispatch_line\n",
                        "    if self.quitting: raise BdbQuit\n",
                        "bdb.BdbQuit\n"
                    ]
                }
            ],
            "source": [
                "from jmp.lightning import Runner, Trainer\n",
                "\n",
                "def count_parameters(model):\n",
                "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "\n",
                "def run(config: PretrainConfig, model_cls: type[PretrainModel]) -> None:\n",
                "    model = model_cls(config)\n",
                "    trainer = Trainer(config)\n",
                "    trainer.fit(model)\n",
                "\n",
                "\n",
                "runner = Runner(run)\n",
                "runner.fast_dev_run(configs, n_batches=16)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "fm",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
