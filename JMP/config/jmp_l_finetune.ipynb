{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='vjri0m7m' trainer=TrainerConfig(optimizer=OptimizerConfig(log_grad_norm=True, gradient_clipping=GradientClippingConfig(value=1.0, algorithm='value')), supports_skip_batch_exception=False, supports_parameter_hooks=False, set_float32_matmul_precision='medium', precision='16-mixed', max_epochs=500, max_time='07:00:00:00', inference_mode=False) meta={'ckpt_path': PosixPath('/data/shared/ishan_stuff/jmp-s.pt'), 'ema_backbone': True} train_dataset=FinetuneLmdbDatasetConfig(src=PosixPath('/home/sanjeevr/MLFF-distill/data/md22/lmdb/Ac-Ala3-NHMe/train'), metadata_path=PosixPath('/home/sanjeevr/MLFF-distill/data/md22/lmdb/Ac-Ala3-NHMe/train/metadata.npz')) val_dataset=FinetuneLmdbDatasetConfig(src=PosixPath('/home/sanjeevr/MLFF-distill/data/md22/lmdb/Ac-Ala3-NHMe/val'), metadata_path=PosixPath('/home/sanjeevr/MLFF-distill/data/md22/lmdb/Ac-Ala3-NHMe/val/metadata.npz')) test_dataset=FinetuneLmdbDatasetConfig(src=PosixPath('/home/sanjeevr/MLFF-distill/data/md22/lmdb/Ac-Ala3-NHMe/test'), metadata_path=PosixPath('/home/sanjeevr/MLFF-distill/data/md22/lmdb/Ac-Ala3-NHMe/test/metadata.npz')) optimizer=AdamWConfig(lr=5e-06, weight_decay=0.1, betas=(0.9, 0.95)) lr_scheduler=WarmupCosRLPConfig(warmup_epochs=5, max_epochs=32, warmup_start_lr_factor=0.1, min_lr_factor=0.1, rlp=RLPConfig(patience=3, factor=0.8)) backbone=BackboneConfig(num_spherical=7, num_radial=128, num_blocks=6, emb_size_atom=256, emb_size_edge=1024, emb_size_trip_in=64, emb_size_trip_out=128, emb_size_quad_in=64, emb_size_quad_out=32, emb_size_aint_in=64, emb_size_aint_out=64, emb_size_rbf=32, emb_size_cbf=16, emb_size_sbf=64, num_before_skip=2, num_after_skip=2, num_concat=4, num_atom=3, num_output_afteratom=3, num_atom_emb_layers=2, regress_forces=False, sbf={'name': 'legendre_outer'}, quad_interaction=True, atom_edge_interaction=True, edge_atom_interaction=True, atom_interaction=True, qint_tags=[1, 2], absolute_rbf_cutoff=12.0, dropout=None, edge_dropout=None) batch_size=4 primary_metric=PrimaryMetricConfig(name='force_mae', mode='min') early_stopping=EarlyStoppingConfig(patience=50, min_lr=1e-08) normalization={'y': NormalizationConfig(mean=-26913.953, std=0.35547638), 'force': NormalizationConfig(mean=0.0, std=1.1291506)} parameter_specific_optimizers=[ParamSpecificOptimizerConfig(paremeter_patterns=['embedding.*'], optimizer=AdamWConfig(lr=1.5e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=5, max_epochs=32, warmup_start_lr_factor=0.1, min_lr_factor=0.33333333333333337, rlp=RLPConfig(patience=3, factor=0.8))), ParamSpecificOptimizerConfig(paremeter_patterns=['backbone.int_blocks.0.*', 'backbone.out_blocks.1.*', 'backbone.out_blocks.0.*'], optimizer=AdamWConfig(lr=2.7500000000000004e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=5, max_epochs=32, warmup_start_lr_factor=0.1, min_lr_factor=0.18181818181818182, rlp=RLPConfig(patience=3, factor=0.8))), ParamSpecificOptimizerConfig(paremeter_patterns=['backbone.int_blocks.1.*', 'backbone.out_blocks.2.*'], optimizer=AdamWConfig(lr=2.0000000000000003e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=5, max_epochs=32, warmup_start_lr_factor=0.1, min_lr_factor=0.25, rlp=RLPConfig(patience=3, factor=0.8))), ParamSpecificOptimizerConfig(paremeter_patterns=['backbone.int_blocks.2.*', 'backbone.out_blocks.3.*'], optimizer=AdamWConfig(lr=1.5e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=5, max_epochs=32, warmup_start_lr_factor=0.1, min_lr_factor=0.33333333333333337, rlp=RLPConfig(patience=3, factor=0.8))), ParamSpecificOptimizerConfig(paremeter_patterns=['backbone.int_blocks.3.*', 'backbone.out_blocks.4.*'], optimizer=AdamWConfig(lr=2.0000000000000003e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=5, max_epochs=32, warmup_start_lr_factor=0.1, min_lr_factor=0.25, rlp=RLPConfig(patience=3, factor=0.8))), ParamSpecificOptimizerConfig(paremeter_patterns=['backbone.int_blocks.4.*', 'backbone.out_blocks.5.*'], optimizer=AdamWConfig(lr=2.7500000000000004e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=5, max_epochs=32, warmup_start_lr_factor=0.1, min_lr_factor=0.18181818181818182, rlp=RLPConfig(patience=3, factor=0.8))), ParamSpecificOptimizerConfig(paremeter_patterns=['backbone.int_blocks.5.*', 'backbone.out_blocks.6.*'], optimizer=AdamWConfig(lr=3.125e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=5, max_epochs=32, warmup_start_lr_factor=0.1, min_lr_factor=0.16, rlp=RLPConfig(patience=3, factor=0.8)))] gradient_forces=True model_type='forces' molecule='Ac-Ala3-NHMe'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "All rights reserved.\n",
    "\n",
    "This source code is licensed under the license found in the\n",
    "LICENSE file in the root directory of this source tree.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from jmp.configs.finetune.jmp_l import jmp_l_ft_config_\n",
    "from jmp.configs.finetune.md22 import jmp_l_md22_config_\n",
    "from jmp.tasks.finetune.base import FinetuneConfigBase, FinetuneModelBase\n",
    "from jmp.tasks.finetune.md22 import MD22Config, MD22Model\n",
    "\n",
    "ckpt_path = Path(\"/data/shared/ishan_stuff/jmp-s.pt\")\n",
    "base_path = Path(\"/home/sanjeevr/MLFF-distill/data/md22\")\n",
    "\n",
    "# We create a list of all configurations that we want to run.\n",
    "configs: list[tuple[FinetuneConfigBase, type[FinetuneModelBase]]] = []\n",
    "\n",
    "config = MD22Config.draft()\n",
    "jmp_l_ft_config_(config, ckpt_path)  # This loads the base JMP-L fine-tuning config\n",
    "# This loads the rMD17-specific configuration\n",
    "jmp_l_md22_config_(config, \"Ac-Ala3-NHMe\", base_path)\n",
    "config = config.finalize()  # Actually construct the config object\n",
    "print(config)\n",
    "\n",
    "configs.append((config, MD22Model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a50c02fb1f34a029b6af567f76d1488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast dev run:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import lovely-tensors. Ignoring pretty PyTorch tensor formatting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import rich. Falling back to default Python logging.\n",
      "CRITICAL:jmp.lightning.trainer.trainer:Setting config.trainer.default_root_dir='/home/sanjeevr/MLFF-distill/JMP/config/lightning_logs/l1gfnmct'.\n",
      "Seed set to 0\n",
      "CRITICAL:jmp.lightning.util.seed:Set global seed to 0.\n",
      "CRITICAL:jmp.lightning.runner:Auto-wrapping run in Trainer context\n",
      "CRITICAL:jmp.tasks.finetune.base:Using regular backbone\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrecognized arguments:  dict_keys(['learnable_rbf', 'learnable_rbf_stds', 'unique_basis_per_layer', 'dropout', 'edge_dropout'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:jmp.tasks.finetune.base:Freezing 0 parameters (0.00%) out of 160,874,752 total parameters (160,874,752 trainable)\n",
      "CRITICAL:jmp.utils.finetune_state_dict:Loaded 405 EMA parameters\n",
      "CRITICAL:jmp.utils.finetune_state_dict:Loaded state dict from /data/shared/ishan_stuff/jmp-s.pt\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.0.scale_rbf_F.*' matched keys ['out_blocks.0.scale_rbf_F.scale_factor'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.0.seq_forces.*' matched keys ['out_blocks.0.seq_forces.0.dense_mlp.0.linear.weight', 'out_blocks.0.seq_forces.0.dense_mlp.1.linear.weight', 'out_blocks.0.seq_forces.1.dense_mlp.0.linear.weight', 'out_blocks.0.seq_forces.1.dense_mlp.1.linear.weight', 'out_blocks.0.seq_forces.2.dense_mlp.0.linear.weight', 'out_blocks.0.seq_forces.2.dense_mlp.1.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.0.dense_rbf_F.*' matched keys ['out_blocks.0.dense_rbf_F.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.1.scale_rbf_F.*' matched keys ['out_blocks.1.scale_rbf_F.scale_factor'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.1.seq_forces.*' matched keys ['out_blocks.1.seq_forces.0.dense_mlp.0.linear.weight', 'out_blocks.1.seq_forces.0.dense_mlp.1.linear.weight', 'out_blocks.1.seq_forces.1.dense_mlp.0.linear.weight', 'out_blocks.1.seq_forces.1.dense_mlp.1.linear.weight', 'out_blocks.1.seq_forces.2.dense_mlp.0.linear.weight', 'out_blocks.1.seq_forces.2.dense_mlp.1.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.1.dense_rbf_F.*' matched keys ['out_blocks.1.dense_rbf_F.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.2.scale_rbf_F.*' matched keys ['out_blocks.2.scale_rbf_F.scale_factor'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.2.seq_forces.*' matched keys ['out_blocks.2.seq_forces.0.dense_mlp.0.linear.weight', 'out_blocks.2.seq_forces.0.dense_mlp.1.linear.weight', 'out_blocks.2.seq_forces.1.dense_mlp.0.linear.weight', 'out_blocks.2.seq_forces.1.dense_mlp.1.linear.weight', 'out_blocks.2.seq_forces.2.dense_mlp.0.linear.weight', 'out_blocks.2.seq_forces.2.dense_mlp.1.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.2.dense_rbf_F.*' matched keys ['out_blocks.2.dense_rbf_F.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.3.scale_rbf_F.*' matched keys ['out_blocks.3.scale_rbf_F.scale_factor'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.3.seq_forces.*' matched keys ['out_blocks.3.seq_forces.0.dense_mlp.0.linear.weight', 'out_blocks.3.seq_forces.0.dense_mlp.1.linear.weight', 'out_blocks.3.seq_forces.1.dense_mlp.0.linear.weight', 'out_blocks.3.seq_forces.1.dense_mlp.1.linear.weight', 'out_blocks.3.seq_forces.2.dense_mlp.0.linear.weight', 'out_blocks.3.seq_forces.2.dense_mlp.1.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.3.dense_rbf_F.*' matched keys ['out_blocks.3.dense_rbf_F.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.4.scale_rbf_F.*' matched keys ['out_blocks.4.scale_rbf_F.scale_factor'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.4.seq_forces.*' matched keys ['out_blocks.4.seq_forces.0.dense_mlp.0.linear.weight', 'out_blocks.4.seq_forces.0.dense_mlp.1.linear.weight', 'out_blocks.4.seq_forces.1.dense_mlp.0.linear.weight', 'out_blocks.4.seq_forces.1.dense_mlp.1.linear.weight', 'out_blocks.4.seq_forces.2.dense_mlp.0.linear.weight', 'out_blocks.4.seq_forces.2.dense_mlp.1.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_blocks.4.dense_rbf_F.*' matched keys ['out_blocks.4.dense_rbf_F.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.utils.state_dict:pattern='out_mlp_F.*' matched keys ['out_mlp_F.out_mlp.0.linear.weight', 'out_mlp_F.out_mlp.1.dense_mlp.0.linear.weight', 'out_mlp_F.out_mlp.1.dense_mlp.1.linear.weight', 'out_mlp_F.out_mlp.2.dense_mlp.0.linear.weight', 'out_mlp_F.out_mlp.2.dense_mlp.1.linear.weight'], which were ignored during loading.\n",
      "CRITICAL:jmp.lightning.trainer.trainer:Ran 0 finalizers for Trainer cleanup.\n",
      "CRITICAL:jmp.lightning.util.seed:Reset global seed.\n",
      "CRITICAL:jmp.lightning.runner:Error in run with run_id='vjri0m7m' (run_name=None): Error(s) in loading state_dict for GemNetOCBackbone:\n",
      "\tsize mismatch for bases.mlp_rbf_qint.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n",
      "\tsize mismatch for bases.mlp_sbf_qint.weight: copying a param with shape torch.Size([128, 49, 32]) from checkpoint, the shape in current model is torch.Size([128, 49, 64]).\n",
      "\tsize mismatch for bases.mlp_rbf_aeint.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n",
      "\tsize mismatch for bases.mlp_rbf_eaint.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n",
      "\tsize mismatch for bases.mlp_rbf_aint.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n",
      "\tsize mismatch for bases.mlp_rbf_tint.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n",
      "\tsize mismatch for bases.mlp_rbf_h.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n",
      "\tsize mismatch for bases.mlp_rbf_out.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n",
      "\tsize mismatch for bases.edge_emb.dense.linear.weight: copying a param with shape torch.Size([512, 640]) from checkpoint, the shape in current model is torch.Size([1024, 640]).\n",
      "\tsize mismatch for int_blocks.0.dense_ca.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.trip_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.trip_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.0.trip_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.0.trip_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.0.trip_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.0.trip_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.0.quad_interaction.dense_db.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.quad_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.0.quad_interaction.mlp_cbf.linear.weight: copying a param with shape torch.Size([32, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n",
      "\tsize mismatch for int_blocks.0.quad_interaction.mlp_sbf.bilinear.linear.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([32, 4096]).\n",
      "\tsize mismatch for int_blocks.0.quad_interaction.down_projection.linear.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.0.quad_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.0.quad_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.0.atom_edge_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([256, 32]).\n",
      "\tsize mismatch for int_blocks.0.atom_edge_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.0.atom_edge_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.0.atom_edge_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.0.edge_atom_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.edge_atom_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.0.edge_atom_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.0.edge_atom_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.0.edge_atom_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n",
      "\tsize mismatch for int_blocks.0.atom_interaction.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([64, 2048]).\n",
      "\tsize mismatch for int_blocks.0.layers_before_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.layers_before_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.layers_before_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.layers_before_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.layers_after_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.layers_after_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.layers_after_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.layers_after_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.atom_update.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.0.atom_update.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for int_blocks.0.concat_layer.dense.linear.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1536]).\n",
      "\tsize mismatch for int_blocks.0.residual_m.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.residual_m.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.dense_ca.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.trip_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.trip_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.1.trip_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.1.trip_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.1.trip_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.1.trip_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.1.quad_interaction.dense_db.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.quad_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.1.quad_interaction.mlp_cbf.linear.weight: copying a param with shape torch.Size([32, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n",
      "\tsize mismatch for int_blocks.1.quad_interaction.mlp_sbf.bilinear.linear.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([32, 4096]).\n",
      "\tsize mismatch for int_blocks.1.quad_interaction.down_projection.linear.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.1.quad_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.1.quad_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.1.atom_edge_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([256, 32]).\n",
      "\tsize mismatch for int_blocks.1.atom_edge_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.1.atom_edge_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.1.atom_edge_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.1.edge_atom_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.edge_atom_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.1.edge_atom_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.1.edge_atom_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.1.edge_atom_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n",
      "\tsize mismatch for int_blocks.1.atom_interaction.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([64, 2048]).\n",
      "\tsize mismatch for int_blocks.1.layers_before_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.layers_before_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.layers_before_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.layers_before_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.layers_after_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.layers_after_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.layers_after_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.layers_after_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.atom_update.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.1.atom_update.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for int_blocks.1.concat_layer.dense.linear.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1536]).\n",
      "\tsize mismatch for int_blocks.1.residual_m.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.residual_m.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.dense_ca.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.trip_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.trip_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.2.trip_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.2.trip_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.2.trip_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.2.trip_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.2.quad_interaction.dense_db.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.quad_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.2.quad_interaction.mlp_cbf.linear.weight: copying a param with shape torch.Size([32, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n",
      "\tsize mismatch for int_blocks.2.quad_interaction.mlp_sbf.bilinear.linear.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([32, 4096]).\n",
      "\tsize mismatch for int_blocks.2.quad_interaction.down_projection.linear.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.2.quad_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.2.quad_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.2.atom_edge_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([256, 32]).\n",
      "\tsize mismatch for int_blocks.2.atom_edge_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.2.atom_edge_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.2.atom_edge_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.2.edge_atom_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.edge_atom_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.2.edge_atom_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.2.edge_atom_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.2.edge_atom_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n",
      "\tsize mismatch for int_blocks.2.atom_interaction.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([64, 2048]).\n",
      "\tsize mismatch for int_blocks.2.layers_before_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.layers_before_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.layers_before_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.layers_before_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.layers_after_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.layers_after_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.layers_after_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.layers_after_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.atom_update.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.2.atom_update.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for int_blocks.2.concat_layer.dense.linear.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1536]).\n",
      "\tsize mismatch for int_blocks.2.residual_m.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.residual_m.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.dense_ca.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.trip_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.trip_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.3.trip_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.3.trip_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.3.trip_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.3.trip_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.3.quad_interaction.dense_db.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.quad_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.3.quad_interaction.mlp_cbf.linear.weight: copying a param with shape torch.Size([32, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n",
      "\tsize mismatch for int_blocks.3.quad_interaction.mlp_sbf.bilinear.linear.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([32, 4096]).\n",
      "\tsize mismatch for int_blocks.3.quad_interaction.down_projection.linear.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.3.quad_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.3.quad_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.3.atom_edge_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([256, 32]).\n",
      "\tsize mismatch for int_blocks.3.atom_edge_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.3.atom_edge_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.3.atom_edge_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.3.edge_atom_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.edge_atom_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.3.edge_atom_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.3.edge_atom_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.3.edge_atom_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n",
      "\tsize mismatch for int_blocks.3.atom_interaction.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([64, 2048]).\n",
      "\tsize mismatch for int_blocks.3.layers_before_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.layers_before_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.layers_before_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.layers_before_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.layers_after_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.layers_after_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.layers_after_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.layers_after_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.atom_update.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.3.atom_update.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for int_blocks.3.concat_layer.dense.linear.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1536]).\n",
      "\tsize mismatch for int_blocks.3.residual_m.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.residual_m.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for out_blocks.0.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for out_blocks.0.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.0.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.1.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for out_blocks.1.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.1.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.2.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for out_blocks.2.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.2.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.3.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for out_blocks.3.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.3.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.4.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for out_blocks.4.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.4.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_mlp_E.out_mlp.0.linear.weight: copying a param with shape torch.Size([256, 1280]) from checkpoint, the shape in current model is torch.Size([256, 1792]).\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sanjeevr/MLFF-distill/JMP/src/jmp/lightning/runner.py\", line 456, in fast_dev_run\n",
      "    return_value = self.local((config, *args), env=env, reset_id=True)\n",
      "  File \"/home/sanjeevr/.local/lib/python3.10/site-packages/typing_extensions.py\", line 2853, in wrapper\n",
      "    return arg(*args, **kwargs)\n",
      "  File \"/home/sanjeevr/MLFF-distill/JMP/src/jmp/lightning/runner.py\", line 198, in local\n",
      "    return_value = self._run_fn(config, *args)\n",
      "  File \"/home/sanjeevr/MLFF-distill/JMP/src/jmp/lightning/runner.py\", line 120, in wrapped_run\n",
      "    return run(config, *args)\n",
      "  File \"/tmp/ipykernel_2313661/3790286775.py\", line 20, in run\n",
      "    model.load_backbone_state_dict(backbone=backbone, embedding=embedding, strict=True)\n",
      "  File \"/home/sanjeevr/MLFF-distill/JMP/src/jmp/tasks/finetune/energy_forces_base.py\", line 102, in load_backbone_state_dict\n",
      "    super().load_backbone_state_dict(\n",
      "  File \"/home/sanjeevr/MLFF-distill/JMP/src/jmp/tasks/finetune/base.py\", line 1055, in load_backbone_state_dict\n",
      "    load_state_dict(\n",
      "  File \"/home/sanjeevr/MLFF-distill/JMP/src/jmp/utils/state_dict.py\", line 87, in load_state_dict\n",
      "    missing_keys, unexpected_keys = module.load_state_dict(state_dict, strict=False)\n",
      "  File \"/home/sanjeevr/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2189, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for GemNetOCBackbone:\n",
      "\tsize mismatch for bases.mlp_rbf_qint.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n",
      "\tsize mismatch for bases.mlp_sbf_qint.weight: copying a param with shape torch.Size([128, 49, 32]) from checkpoint, the shape in current model is torch.Size([128, 49, 64]).\n",
      "\tsize mismatch for bases.mlp_rbf_aeint.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n",
      "\tsize mismatch for bases.mlp_rbf_eaint.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n",
      "\tsize mismatch for bases.mlp_rbf_aint.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n",
      "\tsize mismatch for bases.mlp_rbf_tint.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n",
      "\tsize mismatch for bases.mlp_rbf_h.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n",
      "\tsize mismatch for bases.mlp_rbf_out.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n",
      "\tsize mismatch for bases.edge_emb.dense.linear.weight: copying a param with shape torch.Size([512, 640]) from checkpoint, the shape in current model is torch.Size([1024, 640]).\n",
      "\tsize mismatch for int_blocks.0.dense_ca.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.trip_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.trip_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.0.trip_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.0.trip_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.0.trip_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.0.trip_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.0.quad_interaction.dense_db.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.quad_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.0.quad_interaction.mlp_cbf.linear.weight: copying a param with shape torch.Size([32, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n",
      "\tsize mismatch for int_blocks.0.quad_interaction.mlp_sbf.bilinear.linear.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([32, 4096]).\n",
      "\tsize mismatch for int_blocks.0.quad_interaction.down_projection.linear.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.0.quad_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.0.quad_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.0.atom_edge_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([256, 32]).\n",
      "\tsize mismatch for int_blocks.0.atom_edge_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.0.atom_edge_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.0.atom_edge_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.0.edge_atom_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.edge_atom_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.0.edge_atom_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.0.edge_atom_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.0.edge_atom_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n",
      "\tsize mismatch for int_blocks.0.atom_interaction.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([64, 2048]).\n",
      "\tsize mismatch for int_blocks.0.layers_before_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.layers_before_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.layers_before_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.layers_before_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.layers_after_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.layers_after_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.layers_after_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.layers_after_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.atom_update.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.0.atom_update.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for int_blocks.0.concat_layer.dense.linear.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1536]).\n",
      "\tsize mismatch for int_blocks.0.residual_m.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.0.residual_m.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.dense_ca.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.trip_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.trip_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.1.trip_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.1.trip_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.1.trip_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.1.trip_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.1.quad_interaction.dense_db.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.quad_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.1.quad_interaction.mlp_cbf.linear.weight: copying a param with shape torch.Size([32, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n",
      "\tsize mismatch for int_blocks.1.quad_interaction.mlp_sbf.bilinear.linear.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([32, 4096]).\n",
      "\tsize mismatch for int_blocks.1.quad_interaction.down_projection.linear.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.1.quad_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.1.quad_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.1.atom_edge_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([256, 32]).\n",
      "\tsize mismatch for int_blocks.1.atom_edge_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.1.atom_edge_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.1.atom_edge_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.1.edge_atom_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.edge_atom_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.1.edge_atom_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.1.edge_atom_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.1.edge_atom_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n",
      "\tsize mismatch for int_blocks.1.atom_interaction.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([64, 2048]).\n",
      "\tsize mismatch for int_blocks.1.layers_before_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.layers_before_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.layers_before_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.layers_before_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.layers_after_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.layers_after_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.layers_after_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.layers_after_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.atom_update.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.1.atom_update.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for int_blocks.1.concat_layer.dense.linear.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1536]).\n",
      "\tsize mismatch for int_blocks.1.residual_m.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.1.residual_m.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.dense_ca.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.trip_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.trip_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.2.trip_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.2.trip_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.2.trip_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.2.trip_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.2.quad_interaction.dense_db.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.quad_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.2.quad_interaction.mlp_cbf.linear.weight: copying a param with shape torch.Size([32, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n",
      "\tsize mismatch for int_blocks.2.quad_interaction.mlp_sbf.bilinear.linear.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([32, 4096]).\n",
      "\tsize mismatch for int_blocks.2.quad_interaction.down_projection.linear.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.2.quad_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.2.quad_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.2.atom_edge_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([256, 32]).\n",
      "\tsize mismatch for int_blocks.2.atom_edge_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.2.atom_edge_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.2.atom_edge_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.2.edge_atom_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.edge_atom_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.2.edge_atom_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.2.edge_atom_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.2.edge_atom_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n",
      "\tsize mismatch for int_blocks.2.atom_interaction.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([64, 2048]).\n",
      "\tsize mismatch for int_blocks.2.layers_before_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.layers_before_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.layers_before_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.layers_before_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.layers_after_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.layers_after_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.layers_after_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.layers_after_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.atom_update.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.2.atom_update.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for int_blocks.2.concat_layer.dense.linear.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1536]).\n",
      "\tsize mismatch for int_blocks.2.residual_m.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.2.residual_m.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.dense_ca.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.trip_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.trip_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.3.trip_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.3.trip_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.3.trip_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.3.trip_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.3.quad_interaction.dense_db.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.quad_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.3.quad_interaction.mlp_cbf.linear.weight: copying a param with shape torch.Size([32, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n",
      "\tsize mismatch for int_blocks.3.quad_interaction.mlp_sbf.bilinear.linear.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([32, 4096]).\n",
      "\tsize mismatch for int_blocks.3.quad_interaction.down_projection.linear.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.3.quad_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.3.quad_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.3.atom_edge_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([256, 32]).\n",
      "\tsize mismatch for int_blocks.3.atom_edge_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.3.atom_edge_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.3.atom_edge_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n",
      "\tsize mismatch for int_blocks.3.edge_atom_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.edge_atom_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.3.edge_atom_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n",
      "\tsize mismatch for int_blocks.3.edge_atom_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n",
      "\tsize mismatch for int_blocks.3.edge_atom_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n",
      "\tsize mismatch for int_blocks.3.atom_interaction.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([64, 2048]).\n",
      "\tsize mismatch for int_blocks.3.layers_before_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.layers_before_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.layers_before_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.layers_before_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.layers_after_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.layers_after_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.layers_after_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.layers_after_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.atom_update.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for int_blocks.3.atom_update.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for int_blocks.3.concat_layer.dense.linear.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1536]).\n",
      "\tsize mismatch for int_blocks.3.residual_m.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for int_blocks.3.residual_m.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n",
      "\tsize mismatch for out_blocks.0.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for out_blocks.0.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.0.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.1.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for out_blocks.1.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.1.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.2.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for out_blocks.2.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.2.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.3.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for out_blocks.3.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.3.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.4.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n",
      "\tsize mismatch for out_blocks.4.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_blocks.4.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n",
      "\tsize mismatch for out_mlp_E.out_mlp.0.linear.weight: copying a param with shape torch.Size([256, 1280]) from checkpoint, the shape in current model is torch.Size([256, 1792]).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GemNetOCBackbone:\n\tsize mismatch for bases.mlp_rbf_qint.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n\tsize mismatch for bases.mlp_sbf_qint.weight: copying a param with shape torch.Size([128, 49, 32]) from checkpoint, the shape in current model is torch.Size([128, 49, 64]).\n\tsize mismatch for bases.mlp_rbf_aeint.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n\tsize mismatch for bases.mlp_rbf_eaint.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n\tsize mismatch for bases.mlp_rbf_aint.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n\tsize mismatch for bases.mlp_rbf_tint.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n\tsize mismatch for bases.mlp_rbf_h.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n\tsize mismatch for bases.mlp_rbf_out.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n\tsize mismatch for bases.edge_emb.dense.linear.weight: copying a param with shape torch.Size([512, 640]) from checkpoint, the shape in current model is torch.Size([1024, 640]).\n\tsize mismatch for int_blocks.0.dense_ca.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.trip_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.trip_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.0.trip_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.0.trip_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.0.trip_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.0.trip_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.0.quad_interaction.dense_db.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.quad_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.0.quad_interaction.mlp_cbf.linear.weight: copying a param with shape torch.Size([32, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n\tsize mismatch for int_blocks.0.quad_interaction.mlp_sbf.bilinear.linear.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([32, 4096]).\n\tsize mismatch for int_blocks.0.quad_interaction.down_projection.linear.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.0.quad_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.0.quad_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.0.atom_edge_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([256, 32]).\n\tsize mismatch for int_blocks.0.atom_edge_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.0.atom_edge_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.0.atom_edge_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.0.edge_atom_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.edge_atom_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.0.edge_atom_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.0.edge_atom_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.0.edge_atom_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for int_blocks.0.atom_interaction.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([64, 2048]).\n\tsize mismatch for int_blocks.0.layers_before_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.layers_before_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.layers_before_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.layers_before_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.layers_after_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.layers_after_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.layers_after_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.layers_after_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.atom_update.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.0.atom_update.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for int_blocks.0.concat_layer.dense.linear.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1536]).\n\tsize mismatch for int_blocks.0.residual_m.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.residual_m.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.dense_ca.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.trip_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.trip_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.1.trip_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.1.trip_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.1.trip_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.1.trip_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.1.quad_interaction.dense_db.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.quad_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.1.quad_interaction.mlp_cbf.linear.weight: copying a param with shape torch.Size([32, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n\tsize mismatch for int_blocks.1.quad_interaction.mlp_sbf.bilinear.linear.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([32, 4096]).\n\tsize mismatch for int_blocks.1.quad_interaction.down_projection.linear.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.1.quad_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.1.quad_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.1.atom_edge_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([256, 32]).\n\tsize mismatch for int_blocks.1.atom_edge_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.1.atom_edge_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.1.atom_edge_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.1.edge_atom_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.edge_atom_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.1.edge_atom_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.1.edge_atom_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.1.edge_atom_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for int_blocks.1.atom_interaction.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([64, 2048]).\n\tsize mismatch for int_blocks.1.layers_before_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.layers_before_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.layers_before_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.layers_before_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.layers_after_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.layers_after_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.layers_after_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.layers_after_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.atom_update.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.1.atom_update.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for int_blocks.1.concat_layer.dense.linear.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1536]).\n\tsize mismatch for int_blocks.1.residual_m.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.residual_m.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.dense_ca.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.trip_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.trip_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.2.trip_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.2.trip_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.2.trip_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.2.trip_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.2.quad_interaction.dense_db.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.quad_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.2.quad_interaction.mlp_cbf.linear.weight: copying a param with shape torch.Size([32, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n\tsize mismatch for int_blocks.2.quad_interaction.mlp_sbf.bilinear.linear.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([32, 4096]).\n\tsize mismatch for int_blocks.2.quad_interaction.down_projection.linear.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.2.quad_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.2.quad_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.2.atom_edge_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([256, 32]).\n\tsize mismatch for int_blocks.2.atom_edge_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.2.atom_edge_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.2.atom_edge_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.2.edge_atom_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.edge_atom_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.2.edge_atom_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.2.edge_atom_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.2.edge_atom_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for int_blocks.2.atom_interaction.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([64, 2048]).\n\tsize mismatch for int_blocks.2.layers_before_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.layers_before_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.layers_before_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.layers_before_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.layers_after_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.layers_after_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.layers_after_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.layers_after_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.atom_update.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.2.atom_update.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for int_blocks.2.concat_layer.dense.linear.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1536]).\n\tsize mismatch for int_blocks.2.residual_m.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.residual_m.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.dense_ca.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.trip_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.trip_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.3.trip_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.3.trip_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.3.trip_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.3.trip_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.3.quad_interaction.dense_db.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.quad_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.3.quad_interaction.mlp_cbf.linear.weight: copying a param with shape torch.Size([32, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n\tsize mismatch for int_blocks.3.quad_interaction.mlp_sbf.bilinear.linear.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([32, 4096]).\n\tsize mismatch for int_blocks.3.quad_interaction.down_projection.linear.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.3.quad_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.3.quad_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.3.atom_edge_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([256, 32]).\n\tsize mismatch for int_blocks.3.atom_edge_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.3.atom_edge_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.3.atom_edge_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.3.edge_atom_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.edge_atom_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.3.edge_atom_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.3.edge_atom_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.3.edge_atom_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for int_blocks.3.atom_interaction.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([64, 2048]).\n\tsize mismatch for int_blocks.3.layers_before_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.layers_before_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.layers_before_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.layers_before_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.layers_after_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.layers_after_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.layers_after_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.layers_after_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.atom_update.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.3.atom_update.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for int_blocks.3.concat_layer.dense.linear.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1536]).\n\tsize mismatch for int_blocks.3.residual_m.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.residual_m.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for out_blocks.0.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for out_blocks.0.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.0.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.1.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for out_blocks.1.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.1.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.2.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for out_blocks.2.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.2.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.3.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for out_blocks.3.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.3.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.4.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for out_blocks.4.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.4.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_mlp_E.out_mlp.0.linear.weight: copying a param with shape torch.Size([256, 1280]) from checkpoint, the shape in current model is torch.Size([256, 1792]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mfit(model)\n\u001b[1;32m     26\u001b[0m runner \u001b[38;5;241m=\u001b[39m Runner(run)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_dev_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MLFF-distill/JMP/src/jmp/lightning/runner.py:456\u001b[0m, in \u001b[0;36mRunner.fast_dev_run\u001b[0;34m(self, runs, env, n_batches, stop_on_error)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m     config\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mfast_dev_run \u001b[38;5;241m=\u001b[39m n_batches\n\u001b[0;32m--> 456\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m     return_values\u001b[38;5;241m.\u001b[39mappend(return_value)\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# print full traceback\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/typing_extensions.py:2853\u001b[0m, in \u001b[0;36mdeprecated.__call__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2850\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(arg)\n\u001b[1;32m   2851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2852\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, category\u001b[38;5;241m=\u001b[39mcategory, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MLFF-distill/JMP/src/jmp/lightning/runner.py:198\u001b[0m, in \u001b[0;36mRunner.local\u001b[0;34m(self, env, reset_id, *runs)\u001b[0m\n\u001b[1;32m    196\u001b[0m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mupdate(env)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     return_values\u001b[38;5;241m.\u001b[39mappend(return_value)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/MLFF-distill/JMP/src/jmp/lightning/runner.py:120\u001b[0m, in \u001b[0;36mRunner._run_fn.<locals>.wrapped_run\u001b[0;34m(config, *args)\u001b[0m\n\u001b[1;32m    117\u001b[0m         stack\u001b[38;5;241m.\u001b[39menter_context(Trainer\u001b[38;5;241m.\u001b[39mcontext(config))\n\u001b[1;32m    118\u001b[0m         log\u001b[38;5;241m.\u001b[39mcritical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuto-wrapping run in Trainer context\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExitStack should never raise an exception\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(config, model_cls)\u001b[0m\n\u001b[1;32m     18\u001b[0m embedding \u001b[38;5;241m=\u001b[39m filter_state_dict(state_dict, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding.atom_embedding.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m backbone \u001b[38;5;241m=\u001b[39m filter_state_dict(state_dict, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_backbone_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(config)\n\u001b[1;32m     23\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model)\n",
      "File \u001b[0;32m~/MLFF-distill/JMP/src/jmp/tasks/finetune/energy_forces_base.py:102\u001b[0m, in \u001b[0;36mEnergyForcesModelBase.load_backbone_state_dict\u001b[0;34m(self, backbone, embedding, output, strict)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_backbone_state_dict\u001b[39m(\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m     strict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    101\u001b[0m ):\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_backbone_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretrain_output_head\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    108\u001b[0m             output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    109\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput must be provided when pretrain_output_head is enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/MLFF-distill/JMP/src/jmp/tasks/finetune/base.py:1055\u001b[0m, in \u001b[0;36mFinetuneModelBase.load_backbone_state_dict\u001b[0;34m(self, backbone, embedding, strict)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         ignored_key_patterns\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_blocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.dense_rbf_F.*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1053\u001b[0m         ignored_key_patterns\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_blocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.seq_forces.*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1055\u001b[0m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignored_key_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignored_key_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignored_missing_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_load\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignored_missing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignored_unexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_load\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignored_unexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mckpt_load\u001b[38;5;241m.\u001b[39mreset_embeddings:\n\u001b[1;32m   1064\u001b[0m     load_state_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding, embedding, strict\u001b[38;5;241m=\u001b[39mstrict)\n",
      "File \u001b[0;32m~/MLFF-distill/JMP/src/jmp/utils/state_dict.py:87\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(module, state_dict, ignored_key_patterns, ignored_missing_keys, ignored_unexpected_keys, strict)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pattern, matched_keys \u001b[38;5;129;01min\u001b[39;00m matching_patterns\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     82\u001b[0m         log\u001b[38;5;241m.\u001b[39mcritical(\n\u001b[1;32m     83\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpattern\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m matched keys \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmatched_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich were ignored during loading.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         )\n\u001b[0;32m---> 87\u001b[0m missing_keys, unexpected_keys \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignored_key_patterns:\n\u001b[1;32m     90\u001b[0m     missing_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     91\u001b[0m         k\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(fnmatch\u001b[38;5;241m.\u001b[39mfnmatch(k, pattern) \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m ignored_key_patterns)\n\u001b[1;32m     94\u001b[0m     ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GemNetOCBackbone:\n\tsize mismatch for bases.mlp_rbf_qint.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n\tsize mismatch for bases.mlp_sbf_qint.weight: copying a param with shape torch.Size([128, 49, 32]) from checkpoint, the shape in current model is torch.Size([128, 49, 64]).\n\tsize mismatch for bases.mlp_rbf_aeint.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n\tsize mismatch for bases.mlp_rbf_eaint.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n\tsize mismatch for bases.mlp_rbf_aint.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n\tsize mismatch for bases.mlp_rbf_tint.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n\tsize mismatch for bases.mlp_rbf_h.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n\tsize mismatch for bases.mlp_rbf_out.linear.weight: copying a param with shape torch.Size([16, 128]) from checkpoint, the shape in current model is torch.Size([32, 128]).\n\tsize mismatch for bases.edge_emb.dense.linear.weight: copying a param with shape torch.Size([512, 640]) from checkpoint, the shape in current model is torch.Size([1024, 640]).\n\tsize mismatch for int_blocks.0.dense_ca.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.trip_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.trip_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.0.trip_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.0.trip_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.0.trip_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.0.trip_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.0.quad_interaction.dense_db.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.quad_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.0.quad_interaction.mlp_cbf.linear.weight: copying a param with shape torch.Size([32, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n\tsize mismatch for int_blocks.0.quad_interaction.mlp_sbf.bilinear.linear.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([32, 4096]).\n\tsize mismatch for int_blocks.0.quad_interaction.down_projection.linear.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.0.quad_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.0.quad_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.0.atom_edge_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([256, 32]).\n\tsize mismatch for int_blocks.0.atom_edge_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.0.atom_edge_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.0.atom_edge_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.0.edge_atom_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.edge_atom_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.0.edge_atom_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.0.edge_atom_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.0.edge_atom_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for int_blocks.0.atom_interaction.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([64, 2048]).\n\tsize mismatch for int_blocks.0.layers_before_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.layers_before_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.layers_before_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.layers_before_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.layers_after_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.layers_after_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.layers_after_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.layers_after_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.atom_update.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.0.atom_update.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for int_blocks.0.concat_layer.dense.linear.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1536]).\n\tsize mismatch for int_blocks.0.residual_m.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.0.residual_m.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.dense_ca.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.trip_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.trip_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.1.trip_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.1.trip_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.1.trip_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.1.trip_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.1.quad_interaction.dense_db.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.quad_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.1.quad_interaction.mlp_cbf.linear.weight: copying a param with shape torch.Size([32, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n\tsize mismatch for int_blocks.1.quad_interaction.mlp_sbf.bilinear.linear.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([32, 4096]).\n\tsize mismatch for int_blocks.1.quad_interaction.down_projection.linear.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.1.quad_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.1.quad_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.1.atom_edge_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([256, 32]).\n\tsize mismatch for int_blocks.1.atom_edge_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.1.atom_edge_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.1.atom_edge_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.1.edge_atom_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.edge_atom_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.1.edge_atom_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.1.edge_atom_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.1.edge_atom_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for int_blocks.1.atom_interaction.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([64, 2048]).\n\tsize mismatch for int_blocks.1.layers_before_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.layers_before_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.layers_before_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.layers_before_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.layers_after_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.layers_after_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.layers_after_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.layers_after_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.atom_update.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.1.atom_update.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for int_blocks.1.concat_layer.dense.linear.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1536]).\n\tsize mismatch for int_blocks.1.residual_m.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.1.residual_m.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.dense_ca.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.trip_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.trip_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.2.trip_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.2.trip_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.2.trip_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.2.trip_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.2.quad_interaction.dense_db.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.quad_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.2.quad_interaction.mlp_cbf.linear.weight: copying a param with shape torch.Size([32, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n\tsize mismatch for int_blocks.2.quad_interaction.mlp_sbf.bilinear.linear.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([32, 4096]).\n\tsize mismatch for int_blocks.2.quad_interaction.down_projection.linear.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.2.quad_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.2.quad_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.2.atom_edge_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([256, 32]).\n\tsize mismatch for int_blocks.2.atom_edge_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.2.atom_edge_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.2.atom_edge_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.2.edge_atom_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.edge_atom_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.2.edge_atom_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.2.edge_atom_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.2.edge_atom_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for int_blocks.2.atom_interaction.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([64, 2048]).\n\tsize mismatch for int_blocks.2.layers_before_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.layers_before_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.layers_before_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.layers_before_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.layers_after_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.layers_after_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.layers_after_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.layers_after_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.atom_update.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.2.atom_update.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for int_blocks.2.concat_layer.dense.linear.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1536]).\n\tsize mismatch for int_blocks.2.residual_m.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.2.residual_m.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.dense_ca.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.trip_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.trip_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.3.trip_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.3.trip_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.3.trip_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.3.trip_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.3.quad_interaction.dense_db.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.quad_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.3.quad_interaction.mlp_cbf.linear.weight: copying a param with shape torch.Size([32, 16]) from checkpoint, the shape in current model is torch.Size([64, 16]).\n\tsize mismatch for int_blocks.3.quad_interaction.mlp_sbf.bilinear.linear.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([32, 4096]).\n\tsize mismatch for int_blocks.3.quad_interaction.down_projection.linear.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.3.quad_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.3.quad_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 32]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.3.atom_edge_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([256, 16]) from checkpoint, the shape in current model is torch.Size([256, 32]).\n\tsize mismatch for int_blocks.3.atom_edge_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.3.atom_edge_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.3.atom_edge_interaction.up_projection_ac.linear.weight: copying a param with shape torch.Size([512, 64]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for int_blocks.3.edge_atom_interaction.dense_ba.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.edge_atom_interaction.mlp_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.3.edge_atom_interaction.mlp_cbf.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([128, 1024]).\n\tsize mismatch for int_blocks.3.edge_atom_interaction.down_projection.linear.weight: copying a param with shape torch.Size([64, 512]) from checkpoint, the shape in current model is torch.Size([64, 1024]).\n\tsize mismatch for int_blocks.3.edge_atom_interaction.up_projection_ca.linear.weight: copying a param with shape torch.Size([256, 64]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for int_blocks.3.atom_interaction.bilinear.linear.weight: copying a param with shape torch.Size([64, 1024]) from checkpoint, the shape in current model is torch.Size([64, 2048]).\n\tsize mismatch for int_blocks.3.layers_before_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.layers_before_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.layers_before_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.layers_before_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.layers_after_skip.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.layers_after_skip.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.layers_after_skip.1.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.layers_after_skip.1.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.atom_update.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for int_blocks.3.atom_update.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for int_blocks.3.concat_layer.dense.linear.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1536]).\n\tsize mismatch for int_blocks.3.residual_m.0.dense_mlp.0.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for int_blocks.3.residual_m.0.dense_mlp.1.linear.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for out_blocks.0.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for out_blocks.0.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.0.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.1.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for out_blocks.1.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.1.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.2.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for out_blocks.2.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.2.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.3.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for out_blocks.3.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.3.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.4.dense_rbf.linear.weight: copying a param with shape torch.Size([512, 16]) from checkpoint, the shape in current model is torch.Size([1024, 32]).\n\tsize mismatch for out_blocks.4.layers.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_blocks.4.seq_energy_pre.0.linear.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for out_mlp_E.out_mlp.0.linear.weight: copying a param with shape torch.Size([256, 1280]) from checkpoint, the shape in current model is torch.Size([256, 1792])."
     ]
    }
   ],
   "source": [
    "from jmp.lightning import Runner, Trainer\n",
    "from jmp.utils.finetune_state_dict import (\n",
    "    filter_state_dict,\n",
    "    retreive_state_dict_for_finetuning,\n",
    ")\n",
    "\n",
    "\n",
    "def run(config: FinetuneConfigBase, model_cls: type[FinetuneModelBase]) -> None:\n",
    "    if (ckpt_path := config.meta.get(\"ckpt_path\")) is None:\n",
    "        raise ValueError(\"No checkpoint path provided\")\n",
    "\n",
    "    model = model_cls(config)\n",
    "\n",
    "    # Load the checkpoint\n",
    "    state_dict = retreive_state_dict_for_finetuning(\n",
    "        ckpt_path, load_emas=config.meta.get(\"ema_backbone\", False)\n",
    "    )\n",
    "    embedding = filter_state_dict(state_dict, \"embedding.atom_embedding.\")\n",
    "    backbone = filter_state_dict(state_dict, \"backbone.\")\n",
    "    model.load_backbone_state_dict(backbone=backbone, embedding=embedding, strict=True)\n",
    "\n",
    "    trainer = Trainer(config)\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "runner = Runner(run)\n",
    "runner.fast_dev_run(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
